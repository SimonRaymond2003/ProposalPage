<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Simon Raymond" />

<meta name="date" content="2024-12-04" />

<title>A Predictive and Casual Analysis of Contributing Factors for 4th Down Attempt Conversions in the NFL</title>

<script src="site_libs/header-attrs-2.29/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div id="header">



<h1 class="title toc-ignore">A Predictive and Casual Analysis of
Contributing Factors for 4th Down Attempt Conversions in the NFL</h1>
<h4 class="author">Simon Raymond</h4>
<h4 class="date">2024-12-04</h4>

</div>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<div id="background" class="section level2">
<h2>Background</h2>
<p>The NFL is a multi-billion dollar industry that has seen rapid growth
within the USA and internationally. Davis and End argue that successful
NFL franchises have measurable economic impacts on their local areas.
This growth has placed more and more importance on the performance of
NFL teams as they fight to win games to increase the popularity of their
team. Teams are willing to invest time and money into finding different
strategies that assist their organization. One such area that has
received much attention is the idea of maximizing the expected win
percentages of teams. This leads teams to look to strategies that may be
outside of the culture norm.</p>
<p>For many years in the NFL there seemed to be a consistent standard
that on 4th down your team will kick a field goal or punt the ball for
better field position. The only exception being the dying moments in a
game when teams are desperate. However data display tools have let us
see an increase in the overall attempt percentage of teams on 4th down
(). This is signalling a change in culture</p>
<p>In the NFL there has been a increasing feeling that teams need to be
more aggressive on 4th downs. We have seen teams adopt this strategy.
Most famously is the Detroit lion’s mentality since the arrival of their
current head coach Dan Campbell. The lions adopted a aggressive strategy
to match their aggressive “biting off knee caps” mentality. While the
lions have seen success for the first time in years they have also been
criticized for their aggressive play calling. This was seen in the
2023-24 playoff divisional round game in which the lions failed a 4th
down attempt that was painted as unnecessary . After this failed 4th
down attempt there seemed to be a shift in momentum and the lions lost
the game.</p>
</div>
<div id="research-problem" class="section level2">
<h2>Research Problem</h2>
<p>A result like this makes us ask the question “Did the lions make the
right call?”. This question seems to be getting answered as “yes” by the
current literature on this topic. However, we need to know if different
teams should go for it or not go for it depending on their situation and
team build. For example it could be argued that the lions should have
attempted the crucial 4th down in the 2023-24 playoff divisional round
game. However if the panthers (which were a significantly worse team)
where is that situation it could be argued that they should not have
been as aggressive. This is because the panthers could have had a worse
chance of being able to convert on 4th down.</p>
<p>On top of this we must be weary of any recommendation that is given
to a head coach. The truth is that we are not on the field, in the
locker rooms, or in team meeting. This means coaches may know more then
us in certain game time decision. We must approach this topic with the
idea of being more practical and clear to coaches.</p>
</div>
<div id="research-questions" class="section level2">
<h2>Research Questions</h2>
<p>This leads us to have a need to answer some key questions about 4th
downs in the NFL. First is are there certain key predictable signals
that can be used to decide if a given team should attempt a 4th down.
Second, is what key factors or variables about players have predictive
power in 4th down attempts. In other words are there players that are
more important in 4th down situations when compared to other situations.
Finally. do these key factors about players have a casual effect on the
outcome of 4th down attempts? Answering these questions will allow
coaches to look for key signals in 4th down situations and to know which
players to start on that 4th down if it is decided to attempt. This also
can be applied in discovering specialty players that are overlooked due
to poor performances in situations that are not similar to 4th down.</p>
</div>
</div>
<div id="literature-review" class="section level1">
<h1>Literature Review</h1>
<div id="risk-aversion" class="section level2">
<h2>Risk aversion</h2>
<p>Much of this problem revolves around the idea that NFL coaches are
acting overly averse to risk which is lowering their expected wins.
found that teams had begun to move towards a more conservative or safe
strategy in the NFL. He argues that teams value successful gambles more
then the expected win percentage in a game. He theorizes that the poor
decision making is either due to risk aversion or it is due to poor
information.</p>
<p>To further this point using matching analysis, quantified this
conservative decision-making, finding that teams could gain
approximately 0.4 wins per year by being more aggressive on fourth
downs.</p>
<p> found when revisiting Romer’s framework that Romer’s core findings
are still held to be true. However they argue that overly conservative
calls are not due to poor decision making. Instead they point to risk
aversion as they estimate that coaches are willing to give up two-thirds
of a expected point to avoid the uncertainty of fourth down
attempts.</p>
<p>On top of this, there seems to be evidence that coaches are more
cautious when their job is on the line. found that in the NCAA coaches
are relatively more conservative when they are more likely to be fired.
At the same time they found when a coach was likely to be promoted that
the coach is more aggressive then normal.</p>
</div>
<div id="momentum" class="section level2">
<h2>Momentum</h2>
<p>If a team feels to be “on fire” should they be more aggressive since
they feel to have momentum? The most famous literature in this is on the
fallacy of the “hot hand”. The hot hand is a cognitive bias that leads
people to believe that a person who has a successful outcome is more
likely to have a successful outcome in future attempts. investigated the
“hot hand” and “shooting streaks” in basket ball. They found that both
players and fans believed in the fallacy despite shots being independent
of each other. similarly discovered that fantasy baseball users
gravitated towards “hot” players. At the same time they were unable to
identify a viable hot hand strategy in DraftKings DFS baseball.</p>
<p>Despite these common findings there does seem to be some evidence of
momentum existing in the NFL. defined momentum in the NFL as “the
sustained increase in win probability by a single team over the course
of at least 2 successive changes in possession”. With this definition
they found that streaks of win probability in football are non-random
and are in fact predictable with Artificial Neural Network Models.</p>
<p> looked to identify momentum across and within games in the NFL.
Within-period momentum was found to encourage teams to take more risks.
Negative within-period momentum was in-turn found to encourage teams to
take less risks. It was also discovered that across-period momentum has
a effect only until a within period momentum was established in a
game.</p>
</div>
<div id="heckman-correction" class="section level2">
<h2>Heckman correction</h2>
<p>The Heckman correction is a two-step estimation process that is used
to correct for sample selection bias (). The first step is to estimate
the probability of selection into the sample. The second step is to
estimate the outcome of interest. Heckmans original paper was focused on
applying probit and linear models for this method. However, due to the
non-linearities in NFL data we will be looking to apply a non-parametric
model to our situation. I am currently in the process of looking at
recent literature and applications of self sample selection bias
corrections with non-parametric models.</p>
<p>Here are some papers that I will be looking at:</p>
<ul>
<li></li>
<li></li>
<li></li>
</ul>
</div>
<div id="research-gap" class="section level2">
<h2>Research Gap</h2>
<p>The gap in the current research revolves around the gap in quality
data. Currently we see many studies include team grades or summary
statistics about teams that are playing against each other. This type of
generalization is needed for econometric models that cannot handle large
amounts of variables. However, our non-parametric models will be able to
handle data with thousands of different variables. To take advantage of
this we will have information about every single player that is on the
field when the ball is snapped. This will allow us to have better
prediction power then previous researchers. This lack of work
accomplished with highly specific data then in turn creates a lack in
researchers identifying key variables about different players in the
field. This is why our combined approach of predictive and casual tools
will be so effective. We will be able to see what variable are or are
not having a effect on 4th down conversions.</p>
</div>
</div>
<div id="data" class="section level1">
<h1>Data</h1>
<div id="play-by-play-pbp" class="section level2">
<h2>Play by Play (PBP)</h2>
<p>Our data consists of play by play data from the NFL. This data is
pulled from the nflverse package in R (). The data ranges from 2016 to
2023. The reason for this is that after 2016 the NFL started to put
tracking chips in NFL player’s jerseys. This gives us information after
2016 of what players are on the field for each snap. As a note a weeks
16-18 in 2023 are missing gsis_ids of who was on the field and the data
in 2016 is described as not as reliable.</p>
<p>To create my data I have merged play by play data, participation
data, roster data, and injury data. However, we may create different
versions of our data depending on our models that we choose to use.</p>
<p>First we have a third down data set. This data has the purpose of
allowing us to test our Heckman Variable’s validity in showing that a
variable has no predictive power on a third down conversion. We then use
this to bolster our argument that the variable in question has no
predictive power on a 4th down conversions. We also will have a fourth
down “attempts” data set. This data set will be used to show that our
Heckman Correction variable does have strong prediction power on the
decision to attempt a 4th down or not. (this situation is more
unbalanced). Finally the end goal is to be using data of purely 4th down
attempts to predict the conversion of a play.</p>
<p>While it is subject to change the 3rd down data set has 102,000+ rows
and 1300+ columns. The 4th down attempts data set has 60,000+ rows and
1300+ columns.</p>
</div>
<div id="pro-football-focus-pff" class="section level2">
<h2>Pro Football Focus (PFF)</h2>
<p>Our weekly player data is a combination of reports from from 2016 to
2023. This data is downloaded in the form of different positional
reports. For example, we downloaded the receiving reports labeled as
“Receiving Grades, Receiving Depth, Receiving Concept, and Receiving vs
Scheme”. Each row in these reports would be how a player did in certain
areas covered by that report that week. This however is useless to us as
we needed to merge every report into each other.</p>
<p>We ended up with a data set that is 158,000+ rows and 1,500+ columns.
Each column is a different stat that may or may not apply to a player
(e.g. a receiving grade does not apply to a DE).</p>
<p>So for each row in this engineered data set there is a player, their
basic information, the year and week, and 1,500+ variables of how that
player preformed in a specific week.</p>
</div>
<div id="merging" class="section level2">
<h2>Merging</h2>
<p>The most key aspect of this entire thesis is the merging of these two
datasets. We can start with how in our pbp data we have the gsis_ids of
every player that was on the field at the time of the play. A gisi_id is
the main unique identifier for the player. So we need to take our PFF
data and for each player merge it into the pbp data. The pbp data that
we are merging it into has 9,000+ rows which each represent a 4th down
attempt that a team intended to go for. This creates a data set with
9,000+ rows and 25,000+ columns. The amount of columns is due to the
specificness of the information that we have about players on the field.
As we continue to merge this data and add or drop different types of
variables this number of columns will change. I believe it will increase
as we find new information. As our project proceeds we will narrow down
our specificness as needed.</p>
<p>A problem we are encountering is that the PFF players do not have a
gsis_id. They instead have a pff_id. Unfortunately not all players with
a gsis_id have a available pff_id. On top of that the names between the
data sets do not always match up. Currently I am working on different
methods of matching these players between datasets. There also are
multiple players with the same name. This match I currently believe can
be done off of parts of names, positions, and teams. For the players
that have a pff_id assigned to their gsis_id this matching process is
not needed.</p>
</div>
</div>
<div id="methodology" class="section level1">
<h1>Methodology</h1>
<div id="measuring-accuracy" class="section level2">
<h2>Measuring Accuracy</h2>
<p>In order to measure the accuracy of our models we will be using the
area under the curve (AUC) of the ROC curve. This is a common metric
used in binary classification problems. The ROC curve is a plot of the
true positive rate against the false positive rate for the different
possible thresholds of a confusion table. These values will be tested
and reported over 10,000 tests of bootstrapped test data.</p>
<p>Our methodology involves two main types of tools. These are
predictive tools and casual econometrics tools.</p>
</div>
<div id="predictive-tools" class="section level2">
<h2>Predictive Tools</h2>
<p>These tools are going to be used to predict the outcome of a 4th down
attempt. If these outcomes of these predictions are acceptable we will
identify what variables had predictive power in our non-parametric
models. The models that will be used are Random Forest, GBM boosting,
XGBoosting, and possibly Neural Networks. All boosting models will be
tuned with some form of grid search.</p>
</div>
<div id="casual-econometrics-tools" class="section level2">
<h2>Casual econometrics tools</h2>
<p>After our predictive modeling we will be left with variables that
have prediction power on the outcome of a 4th down attempt. Then we turn
to our casual econometrics tools. Our situation faces one key problem.
There is selection bias around the 4th down attempts. This means that we
never see the outcomes of 4th down attempts that never happened. In
those situations the ball was punted or a field goal was attempted. To
try to control this selection bias we will be using a Heckman
correction. A form of OLS can be pursued with the Heckman correction.
However, we will also be looking to apply some form of non-linear model
to our situation. This is due to the intense non-linearities that are
found in NFL data.</p>
<p>This self selectiong bias is created in our current world by the fact
that the coach is making a decision about “going for it” or not “going
for it” on 4th down. When we control for this we are attempting to
create a world where the 4th down attempts that we see are free from
bias. This is to say it creates a world where we do actaully see all the
4th down attempts that could have happened.</p>
<p>The Heckman correction requires a key exogenous variable that
predicts 4th attempts but not the results of those attempts. The
variable I propose is score differential. The score differential without
a doubt has predicting power on whether or not a team attempts a 4th
down. The question is does it have prediction power on the actual
conversion of the 4th down. I would argue not.</p>
<p>My one fear was originally that score differential signals a “better”
team. Then since that team is “better” the 4th down result will receive
prediction power from the score differential. However, if we account for
how good the team are I believe that argument would not hold up. Even
without accounting for how good teams are I found that the score
differential was almost useless for predicting the 3rd down conversion
result (in this case we used 3rd down plays to stand in for 4th down
conversions). We can examine the following example. Imagine a world
where the panthers (weaker team) are playing the saints (average team).
The panthers are losing by 30 points so the score differential is -30.
Obviously here the panthers probably have a below average chance of
conversion on a 4th down. However on 4th down the teams swap and the
chiefs (better team) take over for the panthers. I don’t think that it
is fair to say that the 30 point deficit will make it harder for the
chiefs to convert on 4th down. Similarly one may say that the Chiefs may
have a tougher time converting on this 4th down because they are playing
in a more desperate situation. The factors though of this desperate
situation can also be accounted for.</p>
<p>This seems to be backed up by the <a
href="#begining-analysis">beginnings of my analysis</a>.</p>
</div>
</div>
<div id="expected-outcomes" class="section level1">
<h1>Expected Outcomes</h1>
<p>Currently we are predicting 3rd down conversions with a AUC that is
in the mid 60s range. This is not incredible but it makes sense as the
baseline predictions only account for some basic situation variables. I
expect to have presentable AUC in the 80s range when we work on our
properly merged 4th down data set. Then i believe that variables will be
identified that will be proven to have a casual effect on 4th down
conversions. In this process however it is important to recognize the
significance of finding a variable to not have prediction power or a
casual effect.</p>
</div>
<div id="timeline" class="section level1">
<h1>Timeline</h1>
<ul>
<li>I will have the data merged by the beginning of the winter 2025
semester (names matching).</li>
<li>We then will be able to move to modeling and analysis.</li>
</ul>
</div>
<div id="begining-analysis" class="section level1">
<h1>Begining Analysis</h1>
<pre class="r"><code>model_3rd3 &lt;- read_csv(&quot;model_3rd3.csv&quot;)</code></pre>
<pre><code>## Rows: 7022 Columns: 20
## ── Column specification ─────────────
## Delimiter: &quot;,&quot;
## dbl (20): week, ydstogo, yardline_100, posteam_timeouts_remaining, defteam_t...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre class="r"><code>model_4th3 &lt;- read_csv(&quot;model_4th3.csv&quot;)</code></pre>
<pre><code>## Rows: 4226 Columns: 19
## ── Column specification ─────────────
## Delimiter: &quot;,&quot;
## dbl (19): week, attempt, ydstogo, yardline_100, minutes_remaining, posteam_t...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<div id="notes-before-proceeding" class="section level2">
<h2>Notes before proceeding</h2>
<ul>
<li>For display purposes most of the code that produces the data and
plots are not shown. However, the code will be made available via my
personal github page ().</li>
<li>The OLS model takes no account for Heteroskedasticity or
Autocorrelation.</li>
<li>The current data ignores how good the teams are (offensive and
defensive). For that reason our prediction power doesn’t seem
incredible. Howeve, with the proper data that we are creating we can
improve our results.</li>
<li>The relationships are definitely non-linear. In some cases though, I
could engineer the data to help OLS. For example the minutes left in the
game could be two columns one that is the half and one that is the time
from 0-30. However, our end goal is to be using a non-linear model.</li>
<li>I added a random (simulated/useless) variable to the data to give us
a reference point of what variables are useless in our random forest
model.</li>
<li>Week 1 was removed from the data due to how we created some of our
variables.</li>
<li>The data is scaled.</li>
<li>the ROCR curves are from one run of random forest (double check if I
don’t need to loop it because it is random forest)</li>
</ul>
</div>
<div id="measures-of-prediction-power-random-forest"
class="section level2">
<h2>Measures of prediction power Random Forest</h2>
<div id="mdi-gininode-purity" class="section level3">
<h3>MDI (gini/node purity)</h3>
<p>As Gini impurity decreases (meaning nodes become more pure), the MDI
value increases.</p>
<p>This measures how good a variable is at promoting node purity during
the splits. This is for the training of the model. This leads MDI to not
always be the best measure. MDI thrives when variables have a lot of
categories, variability or are continuous. For example, a categorical
variable with 10 levels will have a higher MDI than a binary
variable.</p>
<p>That is why the random variable did good in this measure. The splits
in the trees will sometimes use useless variables. This is because it
fits the training data and it is very normal for RF to use bad
predictors (that is the entire point of randomforest). However MDI can
make it appear that unimportant variables are in fact important.</p>
</div>
<div id="mda-mean-descrease-in-accuracy" class="section level3">
<h3>MDA (Mean Descrease in Accuracy)</h3>
<p>This looks are more how the model would do if we removed the variable
in question from the model. A negative MDA means that the model would do
better without the variable. It is more robust and we need to pay more
attention to this rather than MDI.</p>
</div>
<div id="partial-dependency-plots-pdp" class="section level3">
<h3>Partial Dependency Plots (PDP)</h3>
<p>Partial dependency plots in Random Forests show how one variable
affects predictions while accounting for all other variables in a
realistic way. Rather than arbitrarily fixing other variables to
constants, the method uses all actual combinations of other variables
from the training data.</p>
<p>For example, if we want to understand how score differential affects
fourth down decisions, the process works like this: For each potential
score differential value (say -14 points), the model temporarily sets
every play in the dataset to have that score differential while keeping
all other features (like field position, time remaining, etc.) exactly
as they occurred in real games. It then averages all these predictions
to show the isolated effect of being down 14 points.</p>
</div>
<div id="a-caution-with-pdps" class="section level3">
<h3>A Caution with PDPs</h3>
<p>To issue a strong warning however, here is a PDP of a variable that
has absolutely no connection with 3rd down conversions:</p>
<p>Here is the distribution plot of the random variable we are
using:</p>
<pre class="r"><code>rv_plot</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Here is the PDP of the random variable:</p>
<pre class="r"><code># Display the plot
pdp_random</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>This plot will seem confusing. Why does a useless simulated variable
seem to have predictive power. This random variable does not have
predictive power or a casual effect just because of the PDP plot! As we
will see, this random variable has negative MDA. While it has a very
high MDI that is simply due to the specificness of the variable. This
plot displays how the random forest model is using it. Models like
random forest and XGBoosting are designed to give a voice to weak
predictors. The PDP simply shows how the model is using that variable.
The model may be incorrectly picking up on a pattern that is not truly
within our data. This is proven by the variable’s lack of use near to
the center of its probability distribution. However, as we move away
from the from the center of the random variable’s distribion we get more
specificness that the random forest model thinks is useful for
prediction. There are less observations near the tails of our
distribution and therefore more opportunities to make a obscure split to
help our leaf node purity(gini score) in the training process. This
example needs to serve as a strong caution when observing our PDP plots
in the following analysis.</p>
</div>
</div>
<div id="the-data" class="section level2">
<h2>The Data</h2>
<div id="points-about-the-data" class="section level3">
<h3>Points about the data</h3>
<p>The data we are using is from the 2023 NFL season. We ended up
removing the first week of the season due to how we created some of our
variables. In preparing our data for OLS analysis we decided to remove
the temperature and wind variables since they contained NA values. If
the score differential was above 30 or below -30 then we removed that
observation. The non-binary variables were scaled while preserving the
column names.</p>
</div>
</div>
<div id="variable-descriptions" class="section level2">
<h2>Variable Descriptions</h2>
<ul>
<li><code>attempt</code> (4th down data): Binary indicator (1/0)
representing whether a coach elected to attempt a 4th down conversion
rather than punt or attempt a field goal</li>
<li><code>converted</code> (3rd down data): Binary indicator (1/0)
showing if the play resulted in a successful conversion for a first
down</li>
<li><code>down1_pct</code>: Running play percentage on 1st downs so far
in the game</li>
<li><code>down2_pct</code>: Running play percentage on 2nd downs so far
in the game</li>
<li><code>down3_pct</code>: Running play percentage on 3rd downs so far
in the game</li>
<li><code>opp_scss</code> (previously <code>successes</code>): Number of
successful 4th down conversions by the opposing team in their previous
game</li>
<li><code>opp_fails</code> (previously <code>failures</code>): Number of
failed 4th down attempts by the opposing team in their previous
game</li>
<li><code>score_diff</code>: Point differential at the time of the play
(positive values indicate the offensive team is winning)</li>
<li><code>min_rem</code> (previously <code>minutes_remaining</code>):
Minutes remaining in the game</li>
<li><code>ydstogo</code>: Yards needed to achieve a first down</li>
<li><code>yardline_100</code>: Distance in yards from the opponent’s
endzone (e.g., 75 means the ball is on the offensive team’s 25-yard
line)</li>
<li><code>rush</code> (3rd down data only): Binary indicator (1/0) for
whether the play was a rushing attempt or a pass</li>
<li><code>offtimes</code> (previously
<code>posteam_timeouts_remaining</code>): Number of timeouts remaining
for the offensive team</li>
<li><code>deftimes</code> (previously
<code>defteam_timeouts_remaining</code>): Number of timeouts remaining
for the defensive team</li>
<li><code>week</code>: Week number of the NFL season (Week 1
excluded)</li>
<li><code>prep_days</code>: Number of days the team had to prepare for
this game</li>
<li><code>home</code>: Binary indicator (1/0) for whether the offensive
team is playing at home</li>
<li><code>dome</code>: Binary indicator (1/0) for whether the game is
being played in a dome</li>
<li><code>KICKOFF</code>: Binary indicator (1/0) for whether the
offensive team received the ball via kickoff</li>
<li><code>PUNT</code>: Binary indicator (1/0) for whether the offensive
team received the ball via punt Note: If both KICKOFF and PUNT are 0,
the team received possession through another method (e.g.,
turnover)</li>
<li><code>random_var</code>: Simulated random variable included as a
reference point for evaluating variable importance in our models</li>
</ul>
<pre class="r"><code>glimpse(model_3rd3)</code></pre>
<pre><code>## Rows: 6,915
## Columns: 20
## $ converted    &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, …
## $ down1_pct    &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ down2_pct    &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ down3_pct    &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ opp_scss     &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ opp_fails    &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ score_diff   &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ min_rem      &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ ydstogo      &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ yardline_100 &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ rush         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,…
## $ offtimes     &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 2, 2, 2, 2,…
## $ deftimes     &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3,…
## $ week         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…
## $ prep_days    &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ home         &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,…
## $ dome         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ KICKOFF      &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,…
## $ PUNT         &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ random_var   &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;</code></pre>
<pre class="r"><code>glimpse(model_4th3)</code></pre>
<pre><code>## Rows: 4,163
## Columns: 19
## $ attempt      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, …
## $ down1_pct    &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ down2_pct    &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ down3_pct    &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ opp_scss     &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ opp_fails    &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ score_diff   &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ min_rem      &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ ydstogo      &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ yardline_100 &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ offtimes     &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 0, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,…
## $ deftimes     &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,…
## $ week         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…
## $ prep_days    &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;
## $ home         &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,…
## $ dome         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ KICKOFF      &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,…
## $ PUNT         &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,…
## $ random_var   &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;</code></pre>
</div>
<div id="ols" class="section level2">
<h2>OLS</h2>
<div id="th-down-will-the-coach-attempt-to-go-for-it"
class="section level3">
<h3>4th down “will the coach attempt to go for it?”</h3>
<pre class="r"><code># Fit OLS model
OLS_4th &lt;- lm(attempt ~ ., data = model_4th3)
summary(OLS_4th)</code></pre>
<pre><code>## 
## Call:
## lm(formula = attempt ~ ., data = model_4th3)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.7151 -0.2742 -0.1290  0.2232  1.2636 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   0.321665   0.032383   9.933  &lt; 2e-16 ***
## down1_pct     0.007858   0.006530   1.203   0.2289    
## down2_pct    -0.004963   0.006890  -0.720   0.4714    
## down3_pct     0.011405   0.006487   1.758   0.0788 .  
## opp_scss      0.009169   0.006116   1.499   0.1339    
## opp_fails    -0.001308   0.006023  -0.217   0.8281    
## score_diff   -0.093445   0.006343 -14.733  &lt; 2e-16 ***
## min_rem      -0.073849   0.006472 -11.410  &lt; 2e-16 ***
## ydstogo      -0.121609   0.006180 -19.679  &lt; 2e-16 ***
## yardline_100 -0.053069   0.006266  -8.469  &lt; 2e-16 ***
## offtimes     -0.054160   0.008803  -6.152 8.36e-10 ***
## deftimes      0.020711   0.008597   2.409   0.0160 *  
## week          0.002297   0.001162   1.977   0.0481 *  
## prep_days    -0.002884   0.006045  -0.477   0.6334    
## home          0.001589   0.012054   0.132   0.8952    
## dome          0.020580   0.015522   1.326   0.1849    
## KICKOFF      -0.027270   0.018515  -1.473   0.1409    
## PUNT         -0.024252   0.018353  -1.321   0.1864    
## random_var   -0.006351   0.005965  -1.065   0.2871    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3842 on 4144 degrees of freedom
## Multiple R-squared:  0.201,  Adjusted R-squared:  0.1975 
## F-statistic: 57.92 on 18 and 4144 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>vif(OLS_4th)</code></pre>
<pre><code>##    down1_pct    down2_pct    down3_pct     opp_scss    opp_fails   score_diff 
##     1.202624     1.338873     1.186753     1.054833     1.023034     1.134456 
##      min_rem      ydstogo yardline_100     offtimes     deftimes         week 
##     1.181236     1.076950     1.107316     1.237650     1.281292     1.037317 
##    prep_days         home         dome      KICKOFF         PUNT   random_var 
##     1.030535     1.024332     1.023471     2.398000     2.255370     1.003373</code></pre>
</div>
<div id="rd-down-will-the-play-convert-to-a-1st-down"
class="section level3">
<h3>3rd down “will the play convert to a 1st down?”</h3>
<pre class="r"><code># Fit OLS model
OLS_3rd &lt;- lm(converted ~ ., data = model_3rd3)
summary(OLS_3rd)</code></pre>
<pre><code>## 
## Call:
## lm(formula = converted ~ ., data = model_3rd3)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.7277 -0.3986 -0.2124  0.4833  1.2555 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   0.3623399  0.0336891  10.755  &lt; 2e-16 ***
## down1_pct     0.0070500  0.0060850   1.159 0.246667    
## down2_pct    -0.0057824  0.0063937  -0.904 0.365822    
## down3_pct     0.0084054  0.0060313   1.394 0.163471    
## opp_scss      0.0002207  0.0057122   0.039 0.969176    
## opp_fails     0.0001537  0.0056266   0.027 0.978212    
## score_diff   -0.0021870  0.0059106  -0.370 0.711383    
## min_rem       0.0059934  0.0059996   0.999 0.317846    
## ydstogo      -0.1527965  0.0058681 -26.038  &lt; 2e-16 ***
## yardline_100  0.0199008  0.0057668   3.451 0.000562 ***
## rush          0.0820480  0.0137360   5.973 2.44e-09 ***
## offtimes      0.0061106  0.0086247   0.709 0.478657    
## deftimes      0.0024305  0.0089623   0.271 0.786251    
## week         -0.0011351  0.0010727  -1.058 0.290020    
## prep_days    -0.0062438  0.0056286  -1.109 0.267338    
## home          0.0062293  0.0112071   0.556 0.578338    
## dome          0.0047764  0.0144059   0.332 0.740235    
## KICKOFF       0.0143616  0.0172821   0.831 0.405995    
## PUNT         -0.0137044  0.0171643  -0.798 0.424651    
## random_var   -0.0001875  0.0055568  -0.034 0.973087    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4617 on 6895 degrees of freedom
## Multiple R-squared:  0.1127, Adjusted R-squared:  0.1103 
## F-statistic:  46.1 on 19 and 6895 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>vif(OLS_3rd)</code></pre>
<pre><code>##    down1_pct    down2_pct    down3_pct     opp_scss    opp_fails   score_diff 
##     1.201236     1.326194     1.180097     1.058535     1.027062     1.133359 
##      min_rem      ydstogo yardline_100         rush     offtimes     deftimes 
##     1.167745     1.117116     1.078863     1.097724     1.173236     1.210723 
##         week    prep_days         home         dome      KICKOFF         PUNT 
##     1.037015     1.027783     1.018693     1.021203     2.410760     2.251229 
##   random_var 
##     1.001742</code></pre>
</div>
</div>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
<div id="th-down-will-the-coach-attempt-to-go-for-it-1"
class="section level3">
<h3>4th down “will the coach attempt to go for it?”</h3>
<p>Here we display the results on a 4th down random forest model. This
model predicts attempts and is run with a ntree = 1500. We are reporting
Out Of Bag results.</p>
<pre class="r"><code># Print results
print(importance_df)</code></pre>
<pre><code>##        Variable           0           1 MeanDecreaseAccuracy MeanDecreaseGini
## 1       ydstogo 128.6954211 181.4020866         190.93217344        328.19266
## 2    score_diff  63.4364301 107.9529888         112.63877829        176.70189
## 3       min_rem  61.0056581 101.5765995         105.25199391        193.71662
## 4  yardline_100  46.9461824 105.8365029          94.91931385        199.38863
## 5      offtimes  18.6573277  25.3002167          30.05146536         36.28144
## 6      deftimes  19.3428938   5.7543623          20.11665654         26.09367
## 7          week   0.9122297  10.7662704           6.80515534         59.45495
## 8     down2_pct   2.7711750   4.8422392           5.12154985         75.32021
## 9     down1_pct  -0.2013584   6.9014191           4.03224091         78.73647
## 10    down3_pct  -3.9015920   9.3417061           2.18212632         78.79706
## 11      KICKOFF   2.5793693  -0.5953722           1.82663403         13.70897
## 12         home  -3.2321817   4.5390479           0.08590064         14.65089
## 13         dome  -1.5583562   1.5693642          -0.38511568         10.03248
## 14         PUNT  -2.6732859   2.5024066          -0.84729951         12.79672
## 15   random_var  -2.4606327   2.0706019          -0.90344618        107.34069
## 16     opp_scss  -4.3412620   2.7833494          -1.98165242         36.48357
## 17    opp_fails  -6.4845283   5.2951456          -2.60642708         38.78535
## 18    prep_days  -5.3057667   2.2923039          -3.03117420         33.87779</code></pre>
<pre class="r"><code>print(paste(&quot;OOB AUC:&quot;, round(oob_auc, 3)))</code></pre>
<pre><code>## [1] &quot;OOB AUC: 0.882&quot;</code></pre>
<pre class="r"><code># Plot variable importance
varImpPlot(rf4,
           sort = TRUE,
           main = &quot;Variable Importance Plot&quot;,
           n.var = min(20, ncol(model_4th3)-1))</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>rocr_4th</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>This is a very good AUC despite missing how good the teams are. This
makes sense as NFL coaches are trying to make optimal decisions for
their team to win.</p>
<pre class="r"><code>pdp_4th</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Why do we see score differential influencing prediction of attempts
in our random forest model in this way? First we note that if a team is
losing they are more desperate. These more desperate teams are more
likely to go for it on 4th down. On the other hand if a team is winning
then they feel more aggressive. These teams that are winning are also
more likely to be a better team. This can be captured with other
variables as we proceed with our research.</p>
</div>
<div id="rd-down-will-the-play-convert-to-a-1st-down-1"
class="section level3">
<h3>3rd down “will the play convert to a 1st down?”</h3>
<p>Here we display the results of a 3rd down random forest model. This
model predicts conversion and is run with a ntree = 1500. We are
reporting Out Of Bag results.</p>
<pre class="r"><code># Print results
print(importance_df)</code></pre>
<pre><code>##        Variable          0             1 MeanDecreaseAccuracy MeanDecreaseGini
## 1       ydstogo 83.0154997  1.170804e+02          137.4059442        450.93779
## 2          rush  5.3552315  2.943384e+01           25.8840617         60.74959
## 3  yardline_100 12.2173560 -4.896987e+00            6.6315675        316.18257
## 4       min_rem  3.8162185 -2.484411e-01            2.8442419        347.25243
## 5      deftimes  2.2041560 -4.737069e-01            1.4671006         61.84562
## 6     opp_fails -2.2817420  4.023439e+00            0.8424367        122.57897
## 7    score_diff  0.2775008  7.949896e-01            0.7538057        226.08950
## 8     down3_pct  2.4483324 -2.511512e+00            0.1915744        244.13742
## 9       KICKOFF -4.7212419  5.605999e+00           -0.1609615         43.31617
## 10    down2_pct -2.4018046  2.540927e+00           -0.2385492        241.03270
## 11         week  0.7210256 -1.323246e+00           -0.3615343        182.43879
## 12     opp_scss -3.2839828  3.194074e+00           -0.5374814        114.83749
## 13         PUNT -2.2011950  6.999266e-01           -1.3270207         42.57750
## 14    prep_days  0.3901699 -2.790505e+00           -1.5109884        107.86508
## 15         dome -3.2357074  3.400874e-01           -2.2713549         32.36928
## 16         home -3.3354166 -4.922038e-01           -2.8912307         42.65959
## 17   random_var -2.4519896 -1.640570e+00           -2.9330922        346.69584
## 18     offtimes -5.1887242  8.583918e-04           -4.1418247         59.60974
## 19    down1_pct -8.2198007  1.092242e+00           -5.9596816        242.55411</code></pre>
<pre class="r"><code>print(paste(&quot;OOB AUC:&quot;, round(oob_auc, 3)))</code></pre>
<pre><code>## [1] &quot;OOB AUC: 0.668&quot;</code></pre>
<pre class="r"><code># Plot variable importance
varImpPlot(rf3,
           sort = TRUE,
           main = &quot;Variable Importance Plot&quot;,
           n.var = min(20, ncol(model_3rd3)-1))</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>rocr_3rd</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>This is a bad AUC which currently makes sense as it is a baseline.
This example does not have how good the teams or players are.</p>
<pre class="r"><code># Create plot
pdp_3rd &lt;- ggplot(plot_data, aes(x = score_diff, y = probability)) +
 geom_line(color = &quot;#377EB8&quot;, size = 1.2) +
 labs(
   title = &quot;Effect of Score Differential on 3rd Down Conversion Probability&quot;,
   x = &quot;Score Differential (Points)&quot;,
   y = &quot;Probability of 3rd Down Conversion&quot;
 ) +
 theme_minimal() +
 theme(
   plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;),
   panel.grid.minor = element_blank()
 )</code></pre>
<pre class="r"><code>pdp_3rd</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>Why does the RF model use score differential in the way that it does?
If the score differential is 0 then the team is acting as it should. If
a team is winning that team is often overly cautious which is bad in
terms of your performance. If a team is losing then that team may have
to be overly aggressive which is also bad in terms of your performance.
Especially because this type of aggresion is not the calculated form but
it is often the forced kind. These points are once again variables that
we can control for when using score differential as our exogenous
variable in our selection bias correction. For example we have the yards
to go for 4th downs which is a huge indicator of desperation along with
the time left in the game. We can also look to use how far the QB is
throwing the ball or even the play call and formation to control for
desperation.</p>
</div>
</div>
<div id="proof-of-non-linearities" class="section level2">
<h2>Proof of non-linearities</h2>
<p>As we can imagine NFL data is riddled with different types of
non-linearities. For example the relationship between the score
differential and the probability of a 4th down attempt is not linear.
There are jumps in effect around scoring intervals of 3, 7 and 8 due to
the scoring system in the NFL. This leads us to look more at using
non-parametric models in our analysis. While the self sample selection
bias correction was founded as a parametric model there is literature
that supports us using a non-parametric model.</p>
<p>In this following section keep in mind that our OOB AUC for our
random forest model with 4th down data was 0.88. In this case we are
predicting if a team will attempt a 4th down pass or rushing play. If we
see that a linear regression model is unable to compete with our random
forest model that is a sign of non-linearities in the data. In that case
we will go a step further and apply boosting methods.</p>
<p>Here we will display the results of a algorithm that runs the lm()
function which preforms OLS on 1000 bootstrapped samples of the 4th down
data. The reported values are from the test samples that are
bootstrapped.</p>
<pre class="r"><code># Plot results
plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution for Linear Model&quot;, 
    xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;)
abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2)
abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3)
abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<pre class="r"><code># Print summary statistics
cat(&quot;Mean AUC:&quot;, mean(auc), &quot;\n&quot;)</code></pre>
<pre><code>## Mean AUC: 0.7945926</code></pre>
<pre class="r"><code>cat(&quot;95% CI: [&quot;, mean(auc) - 1.96 * sd(auc), &quot;,&quot;, mean(auc) + 1.96 * sd(auc), &quot;]\n&quot;)</code></pre>
<pre><code>## 95% CI: [ 0.7727761 , 0.8164091 ]</code></pre>
<p>We see a significant worse performance then with a random forest
model.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<p>Cook, J. A. (2022). Sample-Selection-Adjusted Random Forests.
International Journal of Data Science and Analytics, 14, 375-388. <a
href="http://dx.doi.org/10.2139/ssrn.4225014"
class="uri">http://dx.doi.org/10.2139/ssrn.4225014</a></p>
<p>Das, M., Newey, W. K., &amp; Vella, F. (2003). Nonparametric
estimation of sample selection models. The Review of Economic Studies,
70(1), 33-58. <a href="https://doi.org/10.1111/1467-937X.00236"
class="uri">https://doi.org/10.1111/1467-937X.00236</a></p>
<p>Gilovich, T., Vallone, R., &amp; Tversky, A. (1985). The hot hand in
basketball: On the misperception of random sequences. Cognitive
Psychology, 17(3), 295-314. <a
href="https://doi.org/10.1016/0010-0285(85)90010-6"
class="uri">https://doi.org/10.1016/0010-0285(85)90010-6</a></p>
<p>Goff, B. L., &amp; Locke, C. (2019). Revisiting Romer: Digging Deeper
Into Influences on NFL Managerial Decisions. Journal of Sports
Economics, 20(5), 673-694. <a
href="https://doi.org/10.1177/1527002518798686"
class="uri">https://doi.org/10.1177/1527002518798686</a></p>
<p>Heckman, J. J. (1979). Sample Selection Bias as a Specification
Error. Econometrica, 47(1), 153-161. <a
href="https://doi.org/10.2307/1912352"
class="uri">https://doi.org/10.2307/1912352</a></p>
<p>Klein, R. W., &amp; Spady, R. H. (1993). An efficient semiparametric
estimator for binary response models. Econometrica, 61(2), 387-421. <a
href="https://doi.org/10.2307/2951556"
class="uri">https://doi.org/10.2307/2951556</a></p>
<p>Lehman, D. W., &amp; Hahn, J. (2013). Momentum and Organizational
Risk Taking: Evidence from the National Football League. Management
Science, 59(4), 852-868. <a
href="https://doi.org/10.1287/mnsc.1120.1574"
class="uri">https://doi.org/10.1287/mnsc.1120.1574</a></p>
<p>Losak, J. M., Stenquist, R., &amp; Lovett, M. (2023). Behavioral
Biases in Daily Fantasy Baseball: The Case of the Hot Hand. Journal of
Sports Economics, 24(3), 352-372. <a
href="https://doi.org/10.1177/15270025221128955"
class="uri">https://doi.org/10.1177/15270025221128955</a></p>
<p>Owens, M. F., &amp; Roach, M. (2018). Decision-making on the hot seat
and the short list: Evidence from college football fourth down
decisions. Journal of Economic Behavior &amp; Organization, 148,
226-245. <a href="https://doi.org/10.1016/j.jebo.2018.02.023"
class="uri">https://doi.org/10.1016/j.jebo.2018.02.023</a></p>
<p>Roebber, P. J., Schultz, D. M., &amp; Colle, B. A. (2022). On the
existence of momentum in professional football. PLOS ONE, 17(6),
e0269604. <a href="https://doi.org/10.1371/journal.pone.0269604"
class="uri">https://doi.org/10.1371/journal.pone.0269604</a></p>
<p>Romer, D. (2006). Do Firms Maximize? Evidence from Professional
Football. Journal of Political Economy, 114(2), 340-365. <a
href="https://doi.org/10.1086/501171"
class="uri">https://doi.org/10.1086/501171</a></p>
<p>Yam, D., &amp; Lopez, M. (2018). Quantifying the Causal Effects of
Conservative Fourth Down Decision Making in the National Football
League. Available at SSRN: <a href="https://ssrn.com/abstract=3114242"
class="uri">https://ssrn.com/abstract=3114242</a> or <a
href="http://dx.doi.org/10.2139/ssrn.3114242"
class="uri">http://dx.doi.org/10.2139/ssrn.3114242</a></p>
<p>Carl, S., Baldwin, B., Sharpe, L., Ho, T., Edwards, J. (2024).
<strong>nflverse</strong>. Play-by-play data for NFL games. Retrieved
from <a
href="https://nflverse.nflverse.com/">https://nflverse.nflverse.com/</a>.</p>
<p>Pro Football Focus (PFF). (2024). NFL Player Performance Data.
Accessed via subscription at <a
href="https://premium.pff.com/nfl/teams/2024/REGPO">https://premium.pff.com/nfl/teams/2024/REGPO</a>.</p>
<p>Raymond, S. (2024). <strong>4th Down Analysis App</strong>.
Interactive dashboard for analyzing 4th down decisions in the NFL.
Available at: <a
href="https://jzmtko-yigit-aydede.shinyapps.io/NFL_4th_Down_App/">https://jzmtko-yigit-aydede.shinyapps.io/NFL_4th_Down_App/</a>.</p>
<p>Sporting News. (2024). Lions’ Dan Campbell criticized for fourth-down
decisions in playoff loss to 49ers. Retrieved from <a
href="https://www.sportingnews.com/us/nfl/news/lions-49ers-dan-campbell-fourth-down-decisions/b54ce7cc05c4d55fc7b46285">https://www.sportingnews.com/us/nfl/news/lions-49ers-dan-campbell-fourth-down-decisions/b54ce7cc05c4d55fc7b46285</a>.</p>
<p>Davis, M. C., &amp; End, C. M. (2010). A winning proposition: The
economic impact of successful National Football League franchises.
<em>Economic Inquiry</em>, 48(1), 39–50. <a
href="https://doi.org/10.1111/j.1465-7295.2008.00124.x"
class="uri">https://doi.org/10.1111/j.1465-7295.2008.00124.x</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
